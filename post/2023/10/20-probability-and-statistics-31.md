# 《概率统计》 day 31

今天是读《概率统计》的逻辑第 31 天，学习充分统计量。

这个概念理解下来似乎是说对于一些统计推断，不需要完整的样本数据，只要某些统计就够了。

之前的例子，参数 $\theta$ 的估计量只需要统计 $T =\sum_{i=1}^n X_i$，每个 $X_i$ 值就算不知道也无所谓，(3, 1.5, 2.1) 也好 (2.4, 2.2, 2.0) 也罢，没区别。

形式化的定义是统计 $T=r(\vec{X})$ 如果条件概率
 $P(\vec{X} = \vec{x}|T=t, \theta)$ 与 $\theta$ 无关。那么统计 $T$ 是参数 $\theta$ 的充分统计量。

这么说有点抽象，如果似然函数可以分解为

$$
f_n(\vec{x}|\theta) = u(\vec{x}) v[r(\vec{x}), \theta]
$$

那么统计 $T=r(\vec{X})$ 就是充分统计量，$u$ 是 $\vec{x}$ 的函数和 $\theta$ 无关，$v$ 是统计 $T=r(\vec{X})$ 和 $\theta$ 的函数，和单个的 $x_i$ 无关。

比如 $X_i$ 是 $p$ 的伯努利分布，那么似然函数可以分解为

$$u(\vec{x})=1$$

和

$$v[r(\vec{x}), p] = p^{r(\vec{x})}(1-p)^{n-r(\vec{x})}$$

所以 $T = \sum_{i=1}^n x_i$ 就是充分统计量。

<!-- 对于可以写成 $f(x|\theta) = a(\theta)b(x)e^{c(\theta)d(x)}$ 的分布，统计 $T = \sum_{i=1}^n d(x)$ 是充分统计量

伯努利分布能写成

$$f(x|p)=e^{log(1-p)} e^{log(\frac{p}{1-p})x}$$

$d(x) = x$ -->

有些统计推断一个 $T$ 不够，需要多个，这时候似然函数分解为

$$
f_n(\vec{x}|\theta) = u(\vec{x}) v[r_1(\vec{x}), ..., r_k(\vec{x}), \theta]
$$

<!-- 所有的 $T_i = r_i(\vec{x})$ 组成 jointly sufficient statistics。 -->

比如 $X_i$ 是伽马分布，似然函数分解为

$$
f(\vec{x}|\alpha, \beta) = [\frac{\beta^\alpha}{\Gamma(\alpha)}]^n (r_1(\vec{x}))^{\alpha - 1} e^{-\beta r_2(\vec{x})}
$$

$r_1(\vec{x}) = \prod_{i=1}^n x_i$

$r_2(\vec{x}) = \sum_{i=1}^n x_i$

最离谱的情况下需要 n 个 $r_i(\vec{x})$，等于是全要。柯西分布就是这样的分布

$$
f(x|\theta) = \frac{1}{\pi [1+(x-\theta)^2]}
$$

这时候可以用顺序统计量，它把随机样本 $X_1, ..., X_n$ 从小到大排序，整理成另一份随机样本 $Y_1, ..., Y_n$，那这个 $Y_1, ..., Y_n$ 叫顺序统计量。

顺序统计量一定是充分统计量（这不是废话么）。

极小充分统计量 $\vec{T} = (T_1, ..., T_k)$ 是充分统计量且 $\vec{T}$ 是其他充分统计量的函数 $\vec{T} = h(\vec{T}^{'})$。

光看这个定义也不知道怎么找极小充分统计量，不过话说回来，正常做实验谁会放着样本数据不要，闲得蛋疼只收集统计数据。

封面图：Twitter 心臓弱眞君 @xinzoruo